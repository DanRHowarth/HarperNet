{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HarperNet Sprint 1 - Story 3\n",
    "\n",
    "## Part 1: Develop Network Architecture\n",
    "* Determine which model to use\n",
    "* Understand whether we need to determine forward pass behaviour\n",
    "* Develop alternatives for FC layers\n",
    "* Explore fine tuning and Feature Extractor\n",
    "\n",
    "## Part 2: Develop training function\n",
    "* Start with Udacity code, understand it and compare with alternatives (PyTorch tutorial)\n",
    "* Understand how to record loss and error, and how to plot this\n",
    "* Develop early stopping\n",
    "\n",
    "## Part 3: Develop inference function\n",
    "* How to display different probabilities \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is an example of CNN architecture as a class, updated and commented\n",
    "## explain everything that is going on here, and try it with a different model - squeezenet\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "\n",
    "class ResNetCNN(nn.Module):\n",
    "    \n",
    "    # class_size relates to the final layer \n",
    "    def __init__(self, class_size):\n",
    "        \n",
    "        # this is necessary - but need to explain this more\n",
    "        # when we initialise, then we combine the variable name with EncoderCNN module\n",
    "        super(ResNetCNN, self).__init__()\n",
    "        \n",
    "        # here we instantiate the restnet50 module\n",
    "        # presumably pretrained gives us our weights?\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        \n",
    "        # interesting that this needs to be a loop rather than resnet.parameters().required_grad_(False)\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad_(False)\n",
    "        \n",
    "        # does order matter? here is what we have done:\n",
    "        # 1. instantiated a model as resnet, with the pretrained weights\n",
    "        # 2. determined that this model does not need training \n",
    "        # 3. created a list of the model layers (or params, or children!) except the last FC layer\n",
    "        # 4. stored this list as the layers in the sequential element of the model \n",
    "        # 5. added a new layer, nn.Linear, which takes in the size of the final fc layer we want in resnet\n",
    "        # and returns the class_size number as output\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        \n",
    "        # note the difference between resent and self.resnet\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.category = nn.Linear(resnet.fc.in_features, class_size)\n",
    "\n",
    "    def forward(self, images):\n",
    "        \n",
    "        # our features is a variable - presumably the output of the original image * weights etc\n",
    "        # - of whatever goes through the resnet layers\n",
    "        features = self.resnet(images)\n",
    "        \n",
    "        # we then need to reshape the output for the linear layers\n",
    "        features = features.view(features.size(0), -1)\n",
    "        \n",
    "        ## we need to perform something here like a softmax? \n",
    "        features = self.category(features)\n",
    "        \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the model \n",
    "class_size = 2                  # add in variable here that will update with the size of class  \n",
    "\n",
    "# creare model as cnn variable\n",
    "cnn = ResNetCNN(class_size)\n",
    "\n",
    "# we can print out the model \n",
    "# print(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False),\n",
       " BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       " ReLU(inplace),\n",
       " MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False),\n",
       " Sequential(\n",
       "   (0): Bottleneck(\n",
       "     (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (downsample): Sequential(\n",
       "       (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "       (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     )\n",
       "   )\n",
       "   (1): Bottleneck(\n",
       "     (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "   )\n",
       "   (2): Bottleneck(\n",
       "     (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "   )\n",
       " ),\n",
       " Sequential(\n",
       "   (0): Bottleneck(\n",
       "     (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (downsample): Sequential(\n",
       "       (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "       (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     )\n",
       "   )\n",
       "   (1): Bottleneck(\n",
       "     (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "   )\n",
       "   (2): Bottleneck(\n",
       "     (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "   )\n",
       "   (3): Bottleneck(\n",
       "     (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "   )\n",
       " ),\n",
       " Sequential(\n",
       "   (0): Bottleneck(\n",
       "     (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (downsample): Sequential(\n",
       "       (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "       (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     )\n",
       "   )\n",
       "   (1): Bottleneck(\n",
       "     (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "   )\n",
       "   (2): Bottleneck(\n",
       "     (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "   )\n",
       "   (3): Bottleneck(\n",
       "     (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "   )\n",
       "   (4): Bottleneck(\n",
       "     (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "   )\n",
       "   (5): Bottleneck(\n",
       "     (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "   )\n",
       " ),\n",
       " Sequential(\n",
       "   (0): Bottleneck(\n",
       "     (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (downsample): Sequential(\n",
       "       (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "       (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     )\n",
       "   )\n",
       "   (1): Bottleneck(\n",
       "     (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "   )\n",
       "   (2): Bottleneck(\n",
       "     (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "   )\n",
       " ),\n",
       " AvgPool2d(kernel_size=7, stride=1, padding=0),\n",
       " Linear(in_features=2048, out_features=1000, bias=True)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can print out the resnet model like this \n",
    "# (list(models.resnet50().children()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying to understand what the final layer activation function should be\n",
    "# (list(models.vgg19().children()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the final layer of the resnet model: [Linear(in_features=2048, out_features=1000, bias=True)]\n",
      "the final layer of the CNN model: [Linear(in_features=2048, out_features=2, bias=True)]\n"
     ]
    }
   ],
   "source": [
    "# here we compare the final two layers of resnet and cnn\n",
    "print('the final layer of the resnet model:', list(models.resnet50().children())[-1:])\n",
    "print('the final layer of the CNN model:', list(cnn.children())[-1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps \n",
    "* Try this with a different model, such as `VGG` and with `Squeezenet`\n",
    "* Look at different options for final layers \n",
    "\n",
    "### widki\n",
    "* It must be the case that when we download a model, we get:\n",
    "    * the model structure - accessible it seems via `children` and `parameters`\n",
    "    * the model weights, but **widki** is how to access them - though if we set `param.requires_grad_(False)` then this stops the weights being trained, so I would suggest that `parameters` somehow gives us access to the weights. But, not training the parameters basically could mean don't access those layers. \n",
    "    * new layers are set to `requires_grad` = true by default\n",
    "        * perhaps through `pre_trained=True`\n",
    "* More on Sequential models \n",
    "* **Understand what loss function to apply on the final layer**\n",
    "    * Is this required in the transfer learning scenario? \n",
    "    * It looks like we need to specify in the optimizer what "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at other options for building a model\n",
    "\n",
    "# in_features must return a value of the layer size in question \n",
    "n_inputs = model.classifier[6].in_features\n",
    "\n",
    "# using Sequential must provide some benefits in how we define our models\n",
    "# this method seems to allow us specify our activation functions alongside the layers - so not class based \n",
    "model.classifier[6] = nn.Sequential(\n",
    "    nn.Linear(n_inputs, 256), nn.ReLU(), nn.Dropout(0.4),\n",
    "    nn.Linear(256, n_classes), nn.LogSoftmax(dim=1))\n",
    "\n",
    "model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the tutorial - final later feature extractor - note no change to final layer function\n",
    "\n",
    "model_conv = torchvision.models.resnet18(pretrained=True)\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized as\n",
    "# opoosed to before - below.\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tutorial - fine tuning \n",
    "\n",
    "model_ft = models.resnet18(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_ft = model_ft.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "* Need to trial and error on the activation function for the final layer, start with a no and see if we need to update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Develop training function\n",
    "\n",
    "* Understand interplay between final layer (e.g. softmax) output and loss function (e.g. nnLoss)\n",
    "* Set up a training and validation loop and a means of recording them\n",
    "* Specify optimizer, criterion and learning rate.\n",
    "    * looks like we can use a LR scheduler - explore\n",
    "        * see tutorial; we pass our optimizer to the scheduler and use it when we call step\n",
    "    * optimizer will need to be set on the parameters we wish to train - some or all \n",
    "    * need to know a bit more about error "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "* explore what this function returns and what the backward calculation is \n",
    "* item() returns a python number we can use as a non-tensor \n",
    "* Cross Entropy Loss: losses are averaged across observations for each minibatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example used from TL tutorial\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we use a softmax layer then we can use \n",
    "nn.NLLLoss # i think..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this returns a loss value \n",
    "loss = criterion(outputs,labels)\n",
    "\n",
    "# loss returns a tensor, loss.item() returns a float\n",
    "type(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "* Various options to be used to update the weights per the the losses\n",
    "* Might be certain rules of thumb that will help us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x13810ee08>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the brackets here give us a generator - maybe initiate the call function?\n",
    "cnn.category.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x13810eb48>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this will give us all the parameters from the model, to be used for fine tuning\n",
    "cnn.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# to be used at a later date, lets just try SGD at first\n",
    "#optimizer = optim.Adam(cnn.category.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example used from TL tutorial\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets set a SGD optimizer, as per egs\n",
    "optimizer_SGD = optim.SGD(cnn.category.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2048])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# this will show us which parameters require training - same as the last layer, so not too interesting?\n",
    "for params in optimizer_SGD.param_groups[0]['params']:\n",
    "    if params.requires_grad:\n",
    "        print(params.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* our optimizer parameters change based on whether we are fine tuning or feature extracting - we only optimize our final layer parameters for the latter, and all from the former. \n",
    "    * I *think* I understand - we could of course optimize for all the layers but most of them are frozen so what would be the point?\n",
    "* What is our best optimizer? and parameters? What are default, good rules of thumb? and what "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Function\n",
    "* add in validation loop\n",
    "* add in ability to move to GPU\n",
    "* learn more about how optimizer and criterion work - what they return, what the different options are, and the rules of thumb for both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic function - we can build on this by passing parameters for the model, the optimizer, the \n",
    "\n",
    "def train(n_epochs):\n",
    "    \n",
    "    # loop over the dataset multiple times\n",
    "    for epoch in range(n_epochs):  \n",
    "\n",
    "        # initiate a running loss total \n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for batch_i, data in enumerate(train_loader):\n",
    "            # get the input images and their corresponding labels\n",
    "            inputs, labels = data       \n",
    "\n",
    "            # zero the parameter (weight) gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass to get outputs\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            # calculate the loss ## what does criterion return? \n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # backward pass to calculate the parameter gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # update the parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # print loss statistics\n",
    "            # to convert loss into a scalar and add it to running_loss, we use .item()\n",
    "            running_loss += loss.item()\n",
    "            if batch_i % 1000 == 999:    # print every 1000 mini-batches\n",
    "                print('Epoch: {}, Batch: {}, Avg. Loss: {}'.format(epoch + 1, batch_i+1, running_loss/1000))\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thoughts...\n",
    "* Think about what we are getting when we train a model..\n",
    "    * the model weights for interference \n",
    "\t* the overall model accuracy and loss, the validation and training versions.\n",
    "\t* there are captured in **widki** what output from the `model` or `train_loader`\n",
    "    * actually, what does `train_loader` return? Lots of documentation to look up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more complicated train function from tutorial\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    \n",
    "    # this must be used to get the time the training started\n",
    "    since = time.time()\n",
    "\n",
    "    # we define as variable the model weights as a dictionary. widki copy.deepcopy \n",
    "    # and why we need to specify it now\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # here we could have put epoch + 1 and left num_epochs alone\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        # this seems like a good way of looping through the different phases\n",
    "        for phase in ['train', 'val']:\n",
    "            \n",
    "            if phase == 'train':\n",
    "                # widki think we must be performing an update to step_size param\n",
    "                # widki how optimizer and scheduler work together \n",
    "                scheduler.step()\n",
    "                \n",
    "                # we need to set the model modes differently \n",
    "                model.train()  # Set model to training mode\n",
    "            \n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            # each train and val phase is a complete turn of the dataloader, batch_size etx.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                \n",
    "                # assume these don't have to be in place if we don't use a GPU\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                \n",
    "                # some of this matches the code above, we don't make predictions as we use only loss\n",
    "                # set_grad_enabled turns gradient calculation on or off - on if phase == Train \n",
    "                # so if not enabled, then go gradient calculation, we pass the model through inputs\n",
    "                # make a prediction and calculate a loss (which might be why we use the with....)\n",
    "                # but nore sure why we wouldn't just run the outputs and preds, then put loss in if phase == train\n",
    "                # see if exploring criterion will help\n",
    "            \n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        # what does this hanging out there?\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights  - where to? \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    # does this return the model and best weights?\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my function as saved in train_validate.py\n",
    "\n",
    "## add in early stopping\n",
    "## add in GPU options\n",
    "import copy\n",
    "\n",
    "def train(n_epochs, train_loader, val_loader, model, optimizer, criterion, print_batch=False, print_epoch=True):\n",
    "\n",
    "    # loop over the dataset multiple times\n",
    "    for epoch in range(n_epochs):  \n",
    "        \n",
    "        print('Epoch {}/{}:'.format(epoch + 1, n_epochs))\n",
    "        print('-' * 10)\n",
    "       \n",
    "        # initiate a running loss total for train and validation sets \n",
    "        running_loss = 0.0\n",
    "        val_running_loss = 0.0\n",
    "        \n",
    "        # initiate a running accuracy total for train and validation sets\n",
    "        running_accuracy = 0.0\n",
    "        val_running_accuracy = 0.0\n",
    "        \n",
    "        # inititate a best accuracy variable and a best model weights dictions\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        best_acc = 0.0\n",
    "        \n",
    "        for batch_i, (inputs, labels) in enumerate(train_loader):\n",
    "            \n",
    "            # do we need to do thi? prepare the net for training\n",
    "            model.train()      \n",
    "\n",
    "            # zero the parameter (weight) gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass to get outputs\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            ## ACCURACY\n",
    "            # get predictions to help determine accuracy\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            \n",
    "            # get the correction predictions total by comparing predicted with actual\n",
    "            correct_predictions = torch.sum(predictions == labels).item()\n",
    "            \n",
    "            # get an accuracy per batch\n",
    "            acc_per_batch = correct_predictions / train_loader.batch_size\n",
    "            \n",
    "            # calculate a running total of accuracy\n",
    "            running_accuracy += correct_predictions\n",
    "            \n",
    "            # and get an average by dividing this by the size of the dataset\n",
    "            running_acc_avg = running_accuracy / (train_loader.batch_size * (batch_i + 1))\n",
    "\n",
    "            ## LOSS\n",
    "            # calculate the loss \n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # backward pass to calculate the parameter gradients\n",
    "            loss.backward()\n",
    "            \n",
    "            # store the loss value as a oython number in a variable \n",
    "            loss_per_batch = loss.item()\n",
    "            \n",
    "            # update the parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # keep a running total of our losses \n",
    "            running_loss += loss_per_batch\n",
    "            \n",
    "            # and an average per batch \n",
    "            running_loss_avg = running_loss / float (batch_i + 1)\n",
    "            \n",
    "            if print_batch:\n",
    "                print('Batch {}: Accuracy: {:.4f}; Loss: {:.4f}'.format(batch_i + 1,acc_per_batch, loss_per_batch))\n",
    "                \n",
    "        if print_epoch:        \n",
    "            print('Loss: {:.4f}; Accuracy: {:.4f}'.format(running_loss_avg, running_acc_avg))  \n",
    "    \n",
    "        for batch_ii, (val_inputs, val_labels) in enumerate(val_loader):\n",
    "            \n",
    "            # no requirement to monitor gradients - LOOK UP\n",
    "            with torch.no_grad():\n",
    "                # so set to eval mode - LOOK UP\n",
    "                model.eval()\n",
    "    \n",
    "                # zero the parameter (weight) gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward pass to get outputs\n",
    "                val_outputs = model(val_inputs)\n",
    "\n",
    "                ## ACCURACY\n",
    "                # get predictions to help determine accuracy\n",
    "                _, val_predictions = torch.max(outputs, 1)\n",
    "\n",
    "                # get the correction predictions total by comparing predicted with actual\n",
    "                val_correct_predictions = torch.sum(val_predictions == val_labels).item()\n",
    "\n",
    "                # get an accuracy per batch\n",
    "                val_acc_per_batch = val_correct_predictions / val_loader.batch_size\n",
    "\n",
    "                # calculate a running total of accuracy\n",
    "                val_running_accuracy += val_correct_predictions\n",
    "\n",
    "                # and get an average by dividing this by the size of the dataset\n",
    "                val_running_acc_avg = val_running_accuracy / (val_loader.batch_size * (batch_ii + 1))\n",
    "\n",
    "                ## LOSS\n",
    "                # calculate the loss  - we don't need to calculate the loss.backward or optimizer step\n",
    "                val_loss = criterion(val_outputs, val_labels)\n",
    "\n",
    "                # store the loss value as a oython number in a variable \n",
    "                val_loss_per_batch = val_loss.item()\n",
    "\n",
    "                # keep a running total of our losses \n",
    "                val_running_loss += val_loss_per_batch\n",
    "\n",
    "                # and an average per batch \n",
    "                val_running_loss_avg = val_running_loss / float (batch_ii + 1)\n",
    "                \n",
    "                if print_batch:\n",
    "                    print('VAL: Batch {}: Accuracy: {:.4f}; Loss: {:.4f}'\n",
    "                          .format(batch_ii + 1, val_acc_per_batch, val_loss_per_batch))\n",
    "\n",
    "            if val_running_acc_avg > best_acc:\n",
    "                bes_acc = val_running_acc_avg\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                # print('Model weights updated')\n",
    "                    \n",
    "        if print_epoch:        \n",
    "            print('VAL: Loss: {:.4f}; Accuracy: {:.4f}'.format(val_running_loss_avg, val_running_acc_avg)) \n",
    "            print()\n",
    "\n",
    "    print('Finished Training')\n",
    "    \n",
    "     # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    print('Best model weights saved')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loop exploration\n",
    "# here we can see that, if for loops are at the same level, they each complete a full loop \n",
    "# before moving to the next for loop. \n",
    "# the top level means that one loop is completed before moving to the next, in this case the other two \n",
    "# loops are completed before we go on to the top loop's second value and start again \n",
    "\n",
    "## the lesson: our val set needs to be on the same level as our train set\n",
    "\n",
    "loop_1 = 0\n",
    "loop_2 = 10\n",
    "\n",
    "for i in range(2):\n",
    "    print ('new loop')\n",
    "\n",
    "    for i in range(10):\n",
    "        loop_1 += 1\n",
    "        print (loop_1)\n",
    "    \n",
    "    print('end loop 1')\n",
    "\n",
    "    for i in range(10):\n",
    "        loop_2 += 1\n",
    "        print (loop_2)\n",
    "    \n",
    "    print('end loop 2')\n",
    "\n",
    "print('end main loop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we get outputs per batch - widki these are our predictions\n",
    "## then pass those outputs to the loss function\n",
    "\n",
    "resnet.eval()\n",
    "\n",
    "for i, (data, labels) in enumerate(train_loader):\n",
    "    # this returns one batch's worth of output values\n",
    "    outputs = resnet(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we then need a way to determine what the predictions are - using the torch.max function\n",
    "import torch\n",
    "\n",
    "# if we just call torch.max on outputs \n",
    "torch.max(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we just call torch.max on outputs with dim = 1 then it returns two values\n",
    "# Returns the maximum value of each row of the input tensor in the given dimension dim. \n",
    "# The second return value is the index location of each maximum value found (argmax).\n",
    "torch.max(outputs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this returns just the predictions\n",
    "_, pred = torch.max(outputs, 1)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing the predicted and actual values\n",
    "* view() returns a different shape for the tensor - looks like we need to specify an integer for this\n",
    "* view_as() returns a different shape for the tensor we perform the method on, and lets us specify another tensor as the desired shape\n",
    "* One of the comparison of true vs predicted is using element-wise equality. We compare pred with label data\n",
    "* We can do this a few ways, similar to numpy.\n",
    "* When we put in our equality comparator, we can just specify labels. This seems to work. The code below seems to use a belt and braces approach, so that if labels and preds are different sizes, we change labels to the size of preds to help with the equality comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = torch.Tensor([2,2,2,0])\n",
    "n2 = torch.Tensor([1,1,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## eq compares two different tensors for equality, elementwise. it returns 1 for T, 0 for false\n",
    "\n",
    "# this will return a comparison of n1 and n2\n",
    "torch.eq(n1,n2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as will this \n",
    "n1.eq(n2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so why do we need to use this code?\n",
    "# View this tensor as the same size as other. self.view_as(other) is equivalent to self.view(other.size()).\n",
    "# Please see view() for more information about view.\n",
    "# Parameters:\tother (torch.Tensor) – The result tensor has the same size as other.\n",
    "\n",
    "labels.size()\n",
    "labels.data.size()\n",
    "labels.data.view_as(pred).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## there seems to be a few ways to then compare with the actuals\n",
    "\n",
    "# correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "pred.eq(labels.data.view_as(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets sum this up \n",
    "sum(n1.eq(n2))\n",
    "\n",
    "# and compare with the len of the data\n",
    "# note we need to use .double() to conver to tensor float, or .item() to convert to python\n",
    "sum(n1.eq(n2)).double() / len(n1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(pred == labels).item() / len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# why use .data? This is an interesitng question\n",
    "# labels.type() and labels.data.type return the same thing - see notes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistics\n",
    "running_loss += loss.item() * inputs.size(0)\n",
    "running_corrects += torch.sum(preds == labels.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this gives is our weights, which we can save as a variable and load later\n",
    "model.state_dict())   \n",
    "    \n",
    "# load best model weights\n",
    "model.load_state_dict(best_model_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cv-nd]",
   "language": "python",
   "name": "conda-env-cv-nd-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
